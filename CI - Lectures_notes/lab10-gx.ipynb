{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "from collections import namedtuple, defaultdict\n",
    "from random import choice\n",
    "from copy import deepcopy\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's a simplified version of the Monte Carlo Tree Search (MCTS) algorithm for Tic-Tac-Toe\n",
    "The basic idea of MCTS is to repeatedly simulate games, selecting moves based on a probability distribution that is updated based on the results of the simulations\n",
    "This process allows the algorithm to learn about the value of different positions in the game, and to make better decisions over time.\n",
    "The algorithm uses a two-level tree, with a root node for each possible board position. The algorithm then repeatedly simulates games from each root node, selecting moves based on a random probability distribution. After each simulation, the algorithm updates the probability distribution for each node based on the result of the simulation. This process is repeated until the time limit is reached, or until a game is completed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "State = namedtuple('State', ['x', 'o'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAGIC = [2, 7, 6, 9, 5, 1, 4, 3, 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_board(pos):\n",
    "    \"\"\"Nicely prints the board\"\"\"\n",
    "    for r in range(3):\n",
    "        for c in range(3):\n",
    "            i = r * 3 + c\n",
    "            if MAGIC[i] in pos.x:\n",
    "                print('X', end='')\n",
    "            elif MAGIC[i] in pos.o:\n",
    "                print('O', end='')\n",
    "            else:\n",
    "                print('.', end='')\n",
    "        print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def win(elements):\n",
    "    \"\"\"Checks is elements is winning\"\"\"\n",
    "    return any(sum(c) == 15 for c in combinations(elements, 3))\n",
    "\n",
    "'''A reward is calculated for each state of each game. \n",
    "The reward is calculated as the evaluation of the final state of the game. \n",
    "In this case, the evaluation is done by the state_value() function, which returns a value between -1 and 1.'''\n",
    "\n",
    "def state_value(pos: State):\n",
    "    \"\"\"Evaluate state: +1 first player wins\"\"\"\n",
    "    if win(pos.x):\n",
    "        return 1\n",
    "    elif win(pos.o):\n",
    "        return -1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_game():\n",
    "    trajectory = list()\n",
    "    state = State(set(), set())\n",
    "    available = set(range(1, 9+1))\n",
    "    while available:\n",
    "        x = choice(list(available))\n",
    "        state.x.add(x)\n",
    "        trajectory.append(deepcopy(state))\n",
    "        available.remove(x)\n",
    "        if win(state.x) or not available:\n",
    "            break\n",
    "\n",
    "        o = choice(list(available))\n",
    "        state.o.add(o)\n",
    "        trajectory.append(deepcopy(state))\n",
    "        available.remove(o)\n",
    "        if win(state.o):\n",
    "            break\n",
    "    return trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d498dd1aa362440fb767c62e2c167872",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "value_dictionary = defaultdict(float)\n",
    "hit_state = defaultdict(int)\n",
    "epsilon = 0.001\n",
    "\n",
    "'''the line final_reward = state_value(trajectory[-1]) calculates the reward of the final move of the game. \n",
    "The line value_dictionary[hashable_state] = value_dictionary[ hashable_state ] + epsilon * (final_reward - value_dictionary[hashable_state]) \n",
    "updates the probability distribution for the current node based on the reward of the final move.'''\n",
    "\n",
    "'''The epsilon constant is being used to control the rate of learning. A smaller value of epsilon will result in slower learning, but a more stable estimate of the state values.'''\n",
    "\n",
    "for steps in tqdm(range(500_000)):\n",
    "    trajectory = random_game()\n",
    "    final_reward = state_value(trajectory[-1])\n",
    "    for state in trajectory:\n",
    "        hashable_state = (frozenset(state.x), frozenset(state.o))\n",
    "        hit_state[hashable_state] += 1\n",
    "        value_dictionary[hashable_state] = value_dictionary[\n",
    "            hashable_state\n",
    "        ] + epsilon * (final_reward - value_dictionary[hashable_state])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((frozenset({1, 2, 5, 7, 8}), frozenset({3, 4, 6, 9})), 0.9162768977439065),\n",
       " ((frozenset({1, 3, 4, 7, 8}), frozenset({2, 5, 6, 9})), 0.915435043471609),\n",
       " ((frozenset({1, 2, 3, 4, 9}), frozenset({5, 6, 7, 8})), 0.9138121288629873),\n",
       " ((frozenset({1, 2, 3, 6, 7}), frozenset({4, 5, 8, 9})), 0.9125089093789029),\n",
       " ((frozenset({2, 6, 7, 8, 9}), frozenset({1, 3, 4, 5})), 0.9123336643739864),\n",
       " ((frozenset({1, 2, 4, 5, 6}), frozenset({3, 7, 8, 9})), 0.9121580683526234),\n",
       " ((frozenset({1, 4, 6, 7, 8}), frozenset({2, 3, 5, 9})), 0.9118940146263526),\n",
       " ((frozenset({1, 5, 7, 8, 9}), frozenset({2, 3, 4, 6})), 0.9115407078597959),\n",
       " ((frozenset({4, 5, 6, 8, 9}), frozenset({1, 2, 3, 7})), 0.9112747983417009),\n",
       " ((frozenset({2, 5, 7, 8, 9}), frozenset({1, 3, 4, 6})), 0.9111859843260269)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(value_dictionary.items(), key=lambda e: e[1], reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5477"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(hit_state)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ci-P-7LqQ3C-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
